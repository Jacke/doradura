use std::collections::HashMap;
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::Arc;
use tokio::sync::RwLock;
use tokio::time::{Duration, Instant};
use url::Url;

/// Maximum number of entries in the metadata cache
/// Prevents unbounded memory growth from URL variations
const MAX_CACHE_SIZE: usize = 10_000;

/// Number of entries to evict when cache is full (10% of max)
const EVICTION_BATCH_SIZE: usize = 1_000;

/// –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à–µ
#[derive(Debug, Clone)]
struct CachedMetadata {
    title: String,
    artist: String,
    #[allow(dead_code)]
    thumbnail_url: Option<String>,
    #[allow(dead_code)]
    duration: Option<u32>,
    #[allow(dead_code)]
    filesize: Option<u64>,
    cached_at: Instant,
}

/// –ö—ç—à –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å TTL
/// Uses RwLock for concurrent reads (most operations are reads)
/// Uses AtomicU64 for hit/miss counters (lock-free)
pub struct MetadataCache {
    cache: Arc<RwLock<HashMap<String, CachedMetadata>>>,
    ttl: Duration,
    hit_count: AtomicU64,
    miss_count: AtomicU64,
}

impl MetadataCache {
    /// –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –∫—ç—à —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º TTL
    pub fn new(ttl: Duration) -> Self {
        Self {
            cache: Arc::new(RwLock::new(HashMap::new())),
            ttl,
            hit_count: AtomicU64::new(0),
            miss_count: AtomicU64::new(0),
        }
    }

    /// –ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∏ —É—Å—Ç–∞—Ä–µ–ª–∏
    /// Uses read lock for better concurrency - multiple readers allowed
    pub async fn get(&self, url: &Url) -> Option<(String, String)> {
        let url_str = url.as_str();

        // First try with read lock (allows concurrent readers)
        {
            let cache = self.cache.read().await;
            if let Some(cached) = cache.get(url_str) {
                if Instant::now().duration_since(cached.cached_at) < self.ttl {
                    self.hit_count.fetch_add(1, Ordering::Relaxed);
                    return Some((cached.title.clone(), cached.artist.clone()));
                }
                // Entry expired - need write lock to remove it
            } else {
                // Entry not found
                self.miss_count.fetch_add(1, Ordering::Relaxed);
                return None;
            }
        }

        // Entry expired - upgrade to write lock and remove
        let mut cache = self.cache.write().await;
        // Double-check after acquiring write lock
        if let Some(cached) = cache.get(url_str) {
            if Instant::now().duration_since(cached.cached_at) < self.ttl {
                // Another thread may have updated it
                self.hit_count.fetch_add(1, Ordering::Relaxed);
                return Some((cached.title.clone(), cached.artist.clone()));
            }
            cache.remove(url_str);
        }

        self.miss_count.fetch_add(1, Ordering::Relaxed);
        None
    }

    /// Evicts entries using random sampling to avoid O(n log n) full sort.
    /// Samples SAMPLE_SIZE entries and removes the oldest EVICTION_BATCH_SIZE from sample.
    /// Called with cache lock already held.
    fn evict_oldest_if_needed(cache: &mut HashMap<String, CachedMetadata>) {
        if cache.len() <= MAX_CACHE_SIZE {
            return;
        }

        use rand::seq::IteratorRandom;
        let mut rng = rand::thread_rng();

        // Sample random entries instead of sorting entire cache
        const SAMPLE_SIZE: usize = 500;
        let sample: Vec<_> = cache
            .iter()
            .choose_multiple(&mut rng, SAMPLE_SIZE.min(cache.len()))
            .into_iter()
            .map(|(k, v)| (k.clone(), v.cached_at))
            .collect();

        // Sort only the sample and remove oldest from it
        let mut sorted_sample = sample;
        sorted_sample.sort_by_key(|(_, time)| *time);

        let to_remove: Vec<_> = sorted_sample
            .iter()
            .take(EVICTION_BATCH_SIZE)
            .map(|(k, _)| k.clone())
            .collect();

        for key in &to_remove {
            cache.remove(key);
        }

        log::info!(
            "üóëÔ∏è Cache LRU eviction (sampled): removed {} entries, cache size now {}",
            to_remove.len(),
            cache.len()
        );
    }

    /// –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
    pub async fn set(&self, url: &Url, title: String, artist: String) {
        // –ù–µ –∫—ç—à–∏—Ä—É–µ–º "Unknown Track", –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–ª–∏ "NA" –≤ artist
        if title.trim().is_empty() || title.trim() == "Unknown Track" {
            log::warn!("Not caching invalid metadata: title='{}'", title);
            return;
        }

        // –ï—Å–ª–∏ artist "NA" –∏–ª–∏ –ø—É—Å—Ç–æ–π - –Ω–µ –∫—ç—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–ª–æ—Ö–∏–µ –¥–∞–Ω–Ω—ã–µ
        if artist.trim() == "NA" || artist.trim().is_empty() {
            log::debug!("Not caching metadata with NA/empty artist for URL: {}", url);
            return;
        }

        let url_str = url.as_str();
        let mut cache = self.cache.write().await;

        // Evict oldest entries if cache is full
        Self::evict_oldest_if_needed(&mut cache);

        cache.insert(
            url_str.to_string(),
            CachedMetadata {
                title,
                artist,
                thumbnail_url: None,
                duration: None,
                filesize: None,
                cached_at: Instant::now(),
            },
        );
    }

    /// –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
    pub async fn set_extended(
        &self,
        url: &Url,
        title: String,
        artist: String,
        thumbnail_url: Option<String>,
        duration: Option<u32>,
        filesize: Option<u64>,
    ) {
        // –ù–µ –∫—ç—à–∏—Ä—É–µ–º "Unknown Track" –∏–ª–∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if title.trim().is_empty() || title.trim() == "Unknown Track" {
            log::warn!("Not caching invalid extended metadata: title='{}'", title);
            return;
        }

        let url_str = url.as_str();
        let mut cache = self.cache.write().await;

        // Evict oldest entries if cache is full
        Self::evict_oldest_if_needed(&mut cache);

        cache.insert(
            url_str.to_string(),
            CachedMetadata {
                title,
                artist,
                thumbnail_url,
                duration,
                filesize,
                cached_at: Instant::now(),
            },
        );
    }

    /// –û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫—ç—à–∞
    pub async fn cleanup(&self) -> usize {
        let mut cache = self.cache.write().await;
        let before = cache.len();
        cache.retain(|_, cached| Instant::now().duration_since(cached.cached_at) < self.ttl);
        let removed = before - cache.len();
        log::debug!("Cleaned up {} expired cache entries", removed);
        removed
    }

    /// –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞ (uses read lock for cache, atomic for counters)
    pub async fn stats(&self) -> CacheStats {
        let cache = self.cache.read().await;
        let hits = self.hit_count.load(Ordering::Relaxed);
        let misses = self.miss_count.load(Ordering::Relaxed);
        let total = hits + misses;
        let hit_rate = if total > 0 {
            (hits as f64 / total as f64) * 100.0
        } else {
            0.0
        };

        CacheStats {
            size: cache.len(),
            hits,
            misses,
            hit_rate,
        }
    }

    /// –û—á–∏—â–∞–µ—Ç –≤–µ—Å—å –∫—ç—à
    pub async fn clear(&self) {
        let mut cache = self.cache.write().await;
        cache.clear();
        self.hit_count.store(0, Ordering::Relaxed);
        self.miss_count.store(0, Ordering::Relaxed);
        log::info!("Cache cleared");
    }
}

/// –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞
#[derive(Debug, Clone)]
pub struct CacheStats {
    pub size: usize,
    pub hits: u64,
    pub misses: u64,
    pub hit_rate: f64,
}

/// –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫—ç—à–∞ (singleton)
static METADATA_CACHE: once_cell::sync::Lazy<MetadataCache> = once_cell::sync::Lazy::new(|| {
    MetadataCache::new(Duration::from_secs(24 * 60 * 60)) // 24 —á–∞—Å–∞
});

/// –ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ None
pub async fn get_cached_metadata(url: &Url) -> Option<(String, String)> {
    METADATA_CACHE.get(url).await
}

/// –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
pub async fn cache_metadata(url: &Url, title: String, artist: String) {
    METADATA_CACHE.set(url, title, artist).await;
}

/// –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à
pub async fn cache_extended_metadata(
    url: &Url,
    title: String,
    artist: String,
    thumbnail_url: Option<String>,
    duration: Option<u32>,
    filesize: Option<u64>,
) {
    METADATA_CACHE
        .set_extended(url, title, artist, thumbnail_url, duration, filesize)
        .await;
}

/// –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
pub async fn get_cache_stats() -> CacheStats {
    METADATA_CACHE.stats().await
}

/// –û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫—ç—à–∞
pub async fn cleanup_cache() -> usize {
    METADATA_CACHE.cleanup().await
}

use crate::storage::db::{get_connection, DbPool};

/// –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ID –∏–∑ URL (–ø–µ—Ä–≤—ã–µ 12 —Å–∏–º–≤–æ–ª–æ–≤ —Ö–µ—à–∞)
fn generate_url_id(url: &str) -> String {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};

    let mut hasher = DefaultHasher::new();
    url.hash(&mut hasher);
    let hash = hasher.finish();
    format!("{:x}", hash)[..12].to_string()
}

/// –°–æ—Ö—Ä–∞–Ω—è–µ—Ç URL –≤ –ë–î –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ID –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ callback_data
///
/// URL —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü–µ url_cache —Å TTL 7 –¥–Ω–µ–π.
/// –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–Ω–æ–ø–∫–∞–º —Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–∂–µ –ø–æ—Å–ª–µ —Ä–µ—Å—Ç–∞—Ä—Ç–∞ –±–æ—Ç–∞.
pub async fn store_url(db_pool: &DbPool, url: &str) -> String {
    let id = generate_url_id(url);
    let ttl_seconds = 7 * 24 * 60 * 60; // 7 –¥–Ω–µ–π - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–æ–ª–≥–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã –∫–Ω–æ–ø–æ–∫ –ø–æ—Å–ª–µ —Ä–µ—Å—Ç–∞—Ä—Ç–∞

    // –í—ã—á–∏—Å–ª—è–µ–º expires_at
    let expires_at = chrono::Utc::now() + chrono::Duration::seconds(ttl_seconds);
    let expires_at_str = expires_at.format("%Y-%m-%d %H:%M:%S").to_string();

    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î (INSERT OR REPLACE –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π)
    match get_connection(db_pool) {
        Ok(conn) => {
            if let Err(e) = conn.execute(
                "INSERT OR REPLACE INTO url_cache (id, url, expires_at) VALUES (?1, ?2, ?3)",
                rusqlite::params![id, url, expires_at_str],
            ) {
                log::warn!("Failed to store URL in cache: {}", e);
            } else {
                log::debug!("Stored URL in DB cache: {} -> {}", id, url);
            }
        }
        Err(e) => {
            log::warn!("Failed to get DB connection for URL cache: {}", e);
        }
    }

    id
}

/// –ü–æ–ª—É—á–∞–µ—Ç URL –ø–æ –∫–æ—Ä–æ—Ç–∫–æ–º—É ID –∏–∑ –ë–î
///
/// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ ID –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –∑–∞–ø–∏—Å—å —É—Å—Ç–∞—Ä–µ–ª–∞.
pub async fn get_url(db_pool: &DbPool, id: &str) -> Option<String> {
    match get_connection(db_pool) {
        Ok(conn) => {
            let now = chrono::Utc::now().format("%Y-%m-%d %H:%M:%S").to_string();

            match conn.query_row(
                "SELECT url FROM url_cache WHERE id = ?1 AND expires_at > ?2",
                rusqlite::params![id, now],
                |row| row.get::<_, String>(0),
            ) {
                Ok(url) => {
                    log::debug!("Retrieved URL from DB cache: {} -> {}", id, url);
                    Some(url)
                }
                Err(rusqlite::Error::QueryReturnedNoRows) => {
                    log::debug!("URL not found in DB cache for ID: {}", id);
                    None
                }
                Err(e) => {
                    log::warn!("Failed to get URL from cache: {}", e);
                    None
                }
            }
        }
        Err(e) => {
            log::warn!("Failed to get DB connection for URL cache: {}", e);
            None
        }
    }
}

/// –û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏ –∏–∑ URL –∫–µ—à–∞ –≤ –ë–î
pub async fn cleanup_url_cache(db_pool: &DbPool) -> usize {
    match get_connection(db_pool) {
        Ok(conn) => {
            let now = chrono::Utc::now().format("%Y-%m-%d %H:%M:%S").to_string();

            match conn.execute("DELETE FROM url_cache WHERE expires_at <= ?1", rusqlite::params![now]) {
                Ok(removed) => {
                    if removed > 0 {
                        log::debug!("Cleaned up {} expired URL cache entries from DB", removed);
                    }
                    removed
                }
                Err(e) => {
                    log::warn!("Failed to cleanup URL cache: {}", e);
                    0
                }
            }
        }
        Err(e) => {
            log::warn!("Failed to get DB connection for URL cache cleanup: {}", e);
            0
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_metadata_cache_new() {
        let cache = MetadataCache::new(Duration::from_secs(60));
        let stats = cache.stats().await;
        assert_eq!(stats.size, 0);
        assert_eq!(stats.hits, 0);
        assert_eq!(stats.misses, 0);
    }

    #[tokio::test]
    async fn test_metadata_cache_set_and_get() {
        let cache = MetadataCache::new(Duration::from_secs(60));
        let url = Url::parse("https://youtube.com/watch?v=test123").unwrap();

        // Set metadata
        cache
            .set(&url, "Test Song".to_string(), "Test Artist".to_string())
            .await;

        // Get metadata
        let result = cache.get(&url).await;
        assert!(result.is_some());
        let (title, artist) = result.unwrap();
        assert_eq!(title, "Test Song");
        assert_eq!(artist, "Test Artist");
    }

    #[tokio::test]
    async fn test_metadata_cache_miss() {
        let cache = MetadataCache::new(Duration::from_secs(60));
        let url = Url::parse("https://youtube.com/watch?v=nonexistent").unwrap();

        let result = cache.get(&url).await;
        assert!(result.is_none());

        let stats = cache.stats().await;
        assert_eq!(stats.misses, 1);
    }

    #[tokio::test]
    async fn test_metadata_cache_hit_miss_counts() {
        let cache = MetadataCache::new(Duration::from_secs(60));
        let url = Url::parse("https://youtube.com/watch?v=test456").unwrap();

        // Miss
        cache.get(&url).await;

        // Set and hit
        cache.set(&url, "Song".to_string(), "Artist".to_string()).await;
        cache.get(&url).await;
        cache.get(&url).await;

        let stats = cache.stats().await;
        assert_eq!(stats.misses, 1);
        assert_eq!(stats.hits, 2);
    }

    #[tokio::test]
    async fn test_metadata_cache_ttl_expiration() {
        let cache = MetadataCache::new(Duration::from_millis(50));
        let url = Url::parse("https://youtube.com/watch?v=expire_test").unwrap();

        cache.set(&url, "Expiring Song".to_string(), "Artist".to_string()).await;

        // Should be present immediately
        let result = cache.get(&url).await;
        assert!(result.is_some());

        // Wait for TTL to expire
        tokio::time::sleep(Duration::from_millis(100)).await;

        // Should be expired now
        let result = cache.get(&url).await;
        assert!(result.is_none());
    }

    #[tokio::test]
    async fn test_metadata_cache_cleanup() {
        let cache = MetadataCache::new(Duration::from_millis(50));

        let url1 = Url::parse("https://youtube.com/watch?v=cleanup1").unwrap();
        let url2 = Url::parse("https://youtube.com/watch?v=cleanup2").unwrap();

        cache.set(&url1, "Song 1".to_string(), "Artist 1".to_string()).await;
        cache.set(&url2, "Song 2".to_string(), "Artist 2".to_string()).await;

        assert_eq!(cache.stats().await.size, 2);

        // Wait for expiration
        tokio::time::sleep(Duration::from_millis(100)).await;

        // Cleanup should remove expired entries
        let removed = cache.cleanup().await;
        assert_eq!(removed, 2);
        assert_eq!(cache.stats().await.size, 0);
    }

    #[tokio::test]
    async fn test_metadata_cache_clear() {
        let cache = MetadataCache::new(Duration::from_secs(60));

        let url = Url::parse("https://youtube.com/watch?v=clear_test").unwrap();
        cache.set(&url, "Song".to_string(), "Artist".to_string()).await;
        cache.get(&url).await;

        assert_eq!(cache.stats().await.size, 1);
        assert_eq!(cache.stats().await.hits, 1);

        cache.clear().await;

        let stats = cache.stats().await;
        assert_eq!(stats.size, 0);
        assert_eq!(stats.hits, 0);
        assert_eq!(stats.misses, 0);
    }

    #[tokio::test]
    async fn test_metadata_cache_does_not_cache_invalid_title() {
        let cache = MetadataCache::new(Duration::from_secs(60));
        let url = Url::parse("https://youtube.com/watch?v=invalid_title").unwrap();

        // Empty title should not be cached
        cache.set(&url, "".to_string(), "Artist".to_string()).await;
        assert!(cache.get(&url).await.is_none());

        // "Unknown Track" should not be cached
        cache.set(&url, "Unknown Track".to_string(), "Artist".to_string()).await;
        assert!(cache.get(&url).await.is_none());
    }

    #[tokio::test]
    async fn test_metadata_cache_does_not_cache_na_artist() {
        let cache = MetadataCache::new(Duration::from_secs(60));
        let url = Url::parse("https://youtube.com/watch?v=na_artist").unwrap();

        // "NA" artist should not be cached
        cache.set(&url, "Good Song".to_string(), "NA".to_string()).await;
        assert!(cache.get(&url).await.is_none());

        // Empty artist should not be cached
        cache.set(&url, "Good Song".to_string(), "".to_string()).await;
        assert!(cache.get(&url).await.is_none());
    }

    #[tokio::test]
    async fn test_metadata_cache_set_extended() {
        let cache = MetadataCache::new(Duration::from_secs(60));
        let url = Url::parse("https://youtube.com/watch?v=extended").unwrap();

        cache
            .set_extended(
                &url,
                "Extended Song".to_string(),
                "Artist".to_string(),
                Some("https://example.com/thumb.jpg".to_string()),
                Some(180),
                Some(5000000),
            )
            .await;

        let result = cache.get(&url).await;
        assert!(result.is_some());
        let (title, artist) = result.unwrap();
        assert_eq!(title, "Extended Song");
        assert_eq!(artist, "Artist");
    }

    #[tokio::test]
    async fn test_metadata_cache_hit_rate() {
        let cache = MetadataCache::new(Duration::from_secs(60));
        let url = Url::parse("https://youtube.com/watch?v=hit_rate").unwrap();

        // 1 miss
        cache.get(&url).await;

        // Set and get 3 times (3 hits)
        cache.set(&url, "Song".to_string(), "Artist".to_string()).await;
        cache.get(&url).await;
        cache.get(&url).await;
        cache.get(&url).await;

        let stats = cache.stats().await;
        // 3 hits, 1 miss = 75% hit rate
        assert_eq!(stats.hits, 3);
        assert_eq!(stats.misses, 1);
        assert!((stats.hit_rate - 75.0).abs() < 0.01);
    }

    #[test]
    fn test_generate_url_id() {
        let id1 = generate_url_id("https://youtube.com/watch?v=abc123");
        let id2 = generate_url_id("https://youtube.com/watch?v=def456");

        // IDs should be 12 characters
        assert_eq!(id1.len(), 12);
        assert_eq!(id2.len(), 12);

        // Different URLs should generate different IDs
        assert_ne!(id1, id2);

        // Same URL should generate same ID
        let id1_again = generate_url_id("https://youtube.com/watch?v=abc123");
        assert_eq!(id1, id1_again);
    }

    #[test]
    fn test_generate_url_id_deterministic() {
        let url = "https://example.com/video/12345";
        let id1 = generate_url_id(url);
        let id2 = generate_url_id(url);
        let id3 = generate_url_id(url);

        assert_eq!(id1, id2);
        assert_eq!(id2, id3);
    }

    #[test]
    fn test_cache_stats_debug() {
        let stats = CacheStats {
            size: 10,
            hits: 100,
            misses: 20,
            hit_rate: 83.33,
        };
        let debug_str = format!("{:?}", stats);
        assert!(debug_str.contains("CacheStats"));
        assert!(debug_str.contains("10"));
        assert!(debug_str.contains("100"));
    }

    #[test]
    fn test_cache_stats_clone() {
        let stats = CacheStats {
            size: 5,
            hits: 50,
            misses: 10,
            hit_rate: 83.33,
        };
        let cloned = stats.clone();
        assert_eq!(stats.size, cloned.size);
        assert_eq!(stats.hits, cloned.hits);
        assert_eq!(stats.misses, cloned.misses);
    }
}
